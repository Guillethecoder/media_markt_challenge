{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7dee8658",
      "metadata": {
        "id": "7dee8658"
      },
      "source": [
        "# __Welcome to the MMS Data Engineering Challenge!__\n",
        "\n",
        "This notebook presents a series of problems for you to solve. Note that some problems do not have a single optimal solution, but rather multiple good solutions. We are interested in seeing your approach to these problems. You have considerable flexibility in how you address them. As a Data Engineer, we value your versatility, so solutions may include Python scripts, SQL queries, plain text explanations, or even diagrams.\n",
        "\n",
        "Some exercises are much simpler than others. If you get stuck, feel free to skip the problem and move on. __We will also accept partially completed challenges.__ We want to understand your thought process and the tools you are familiar with. If you don't know how to code a solution but understand how to solve it conceptually, use plain text to explain your answer.\n",
        "\n",
        "In the container folder you will also find the file `sql_definitions.sql` containing all sql table definitions shown bellow. And the original image of the tables in exercises 3 and 4: `duplicates.png` and `employees.png`.\n",
        "\n",
        "**DISCLAIMER**: This Data Engineering Challenge is disguised as a Jupyter Notebook, but don't let that fool youâ€”it's only relevant for the first exercise. For the remaining exercises, feel free to use the code cells or convert them to Markdown cells as you prefer."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guillermo S. Sanjuan Ortiz"
      ],
      "metadata": {
        "id": "mWi-yyd6zxyQ"
      },
      "id": "mWi-yyd6zxyQ"
    },
    {
      "cell_type": "markdown",
      "id": "019f3aad",
      "metadata": {
        "id": "019f3aad"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Write a Python function `zeros_and_ones(n, o)` to print every possible combination of n - o zeros and o ones, for some given n and o.\n",
        "_Note that you have to create at least the mentioned function, but you can create others if needed._\n",
        "\n",
        "__Input__: The input consists of two natural numbers n and o, such that n > 0 and 0 â‰¤ o â‰¤ n.\n",
        "\n",
        "__Output__: Print all the combinations of n âˆ’ o zeros and o ones, one per line and in lexicographical order.\n",
        "\n",
        "__Example__\n",
        "\n",
        "```\n",
        "> zeros_and_ones(5, 2)\n",
        "\n",
        "0 0 0 1 1\n",
        "0 0 1 0 1\n",
        "0 0 1 1 0\n",
        "0 1 0 0 1\n",
        "0 1 0 1 0\n",
        "0 1 1 0 0\n",
        "1 0 0 0 1\n",
        "1 0 0 1 0\n",
        "1 0 1 0 0\n",
        "1 1 0 0 0\n",
        "```\n",
        "\n",
        "The solution is preferred in Python, but in case you don't feel confident enough using Python, provide the solution in another programming language of your choice."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def zeros_and_ones(n:int, o:int)->None:\n",
        "  def print_track(n:int, cars_position:list)->None:\n",
        "    \"\"\"\n",
        "    Prints the track, based on the position of cars\n",
        "    \"\"\"\n",
        "    track = ['0'] * n\n",
        "    for i in cars_position:\n",
        "      track[i] = '1'\n",
        "    print(' '.join(track))\n",
        "    return\n",
        "  def race_continue(n:int, cars_position:list)->list[int]|None:\n",
        "    \"\"\"\n",
        "    Given current position of the cars, calculates the next move and returns next\n",
        "    position of the cars. If race ends, returns None.\n",
        "    \"\"\"\n",
        "    new_cars_position = cars_position.copy()\n",
        "    for i in range(len(cars_position) -1, 0, -1): # Loop over all, but participant one\n",
        "      if cars_position[i] - cars_position[i-1]  > 1: # If they have space ahead move\n",
        "            new_cars_position[i] -= 1\n",
        "            return new_cars_position\n",
        "    if new_cars_position[0] == 0: # If parcitipant one is at the line and no one moves\n",
        "      return # then race ends\n",
        "    else: # If no one moves, means they are all packed so:\n",
        "      new_cars_position[0] -= 1 # Participant one moves\n",
        "      for j in range(1, len(new_cars_position)): # The rest return to initial step\n",
        "          new_cars_position[j] = n - (len(new_cars_position) - j)\n",
        "    return new_cars_position\n",
        "  if n < 0 or o < 0 or o > n: # If input is invalid, raise error\n",
        "    raise ValueError(\"Invalid input\")\n",
        "  if o == 0: # Edge case, no racers\n",
        "    print(' '.join(['0']*n))\n",
        "    return\n",
        "  # Initial cars position\n",
        "  cars_position = list(range(n - o, n))\n",
        "  while cars_position: # Loop continues until cars_position is null\n",
        "    print_track(n, cars_position)\n",
        "    cars_position = race_continue(n, cars_position)"
      ],
      "metadata": {
        "id": "W3DYodKB_DYR"
      },
      "id": "W3DYodKB_DYR",
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9367f7c4",
      "metadata": {
        "id": "9367f7c4"
      },
      "source": [
        "This is just to run your code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "id": "fc8e39c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc8e39c0",
        "outputId": "dd790ee3-b7ad-469f-bbdc-d197ccd919f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 0 1 1\n",
            "0 0 1 0 1\n",
            "0 0 1 1 0\n",
            "0 1 0 0 1\n",
            "0 1 0 1 0\n",
            "0 1 1 0 0\n",
            "1 0 0 0 1\n",
            "1 0 0 1 0\n",
            "1 0 1 0 0\n",
            "1 1 0 0 0\n"
          ]
        }
      ],
      "source": [
        "zeros_and_ones(5, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comments:\n",
        "\n",
        "I thought of it like a race of n steps and o participants, where the first one starts in oth position, the second one in o + 1th position, and so on.\n",
        "\n",
        "They move as soon as they have space ahead, but only if the previous racer have moved, so I loop in inverse, starting with last participant.\n",
        "\n",
        "I don't loop over car 1 because he can always move, but if no one moves, then participant one moves and the others return to the starting point, then the loop begins again until no one has moved and participant is on the line (pos 0).\n",
        "\n",
        "In my implementation, I focus on participants, so I made a `print_track` helper function to print the track in every move.\n",
        "\n",
        "I also created a function `race_continue` to calculate next move, that returns the next step of the cars.\n",
        "\n",
        "This exercise was the hardest one in my opinion.\n",
        "\n"
      ],
      "metadata": {
        "id": "ECY4_FCWXk0z"
      },
      "id": "ECY4_FCWXk0z"
    },
    {
      "cell_type": "markdown",
      "id": "9c9154ce",
      "metadata": {
        "id": "9c9154ce"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "How can the following SQL statement be re-written more easy?\n",
        "\n",
        "```\n",
        "SELECT product_id, IF(has_sales=TRUE, TRUE, FALSE) as purchase_flag,\n",
        "FROM salesdata\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Response\n",
        "```\n",
        "SELECT\n",
        "  product_id,\n",
        "  has_sales AS purchase_flag\n",
        "FROM\n",
        "  salesdata\n",
        "```"
      ],
      "metadata": {
        "id": "o41XIiPHOFWI"
      },
      "id": "o41XIiPHOFWI"
    },
    {
      "cell_type": "markdown",
      "id": "98b21f2a",
      "metadata": {
        "id": "98b21f2a"
      },
      "source": [
        "## Exercise 3\n",
        "\n",
        "There are several ways to find duplicated rows in a table - write a SQL statement of your choice to find duplicates in table C."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06a7903f",
      "metadata": {
        "id": "06a7903f"
      },
      "source": [
        "Table C\n",
        "<img src=\"duplicates.png\" width=\"200px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Response\n",
        "```\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  outlets_info\n",
        "GROUP BY\n",
        "  outlet_name,\n",
        "  country,\n",
        "  region,\n",
        "  address,\n",
        "  zipcode\n",
        "HAVING\n",
        "  COUNT(*) > 1\n",
        "```"
      ],
      "metadata": {
        "id": "2hmPBaXoN-3E"
      },
      "id": "2hmPBaXoN-3E"
    },
    {
      "cell_type": "markdown",
      "id": "d4de094f",
      "metadata": {
        "id": "d4de094f"
      },
      "source": [
        "## Exercise 4\n",
        "EmployeeID, Department, name, salary\n",
        "\n",
        "1. Write a SQL query to get the third-highest salary of an employee from `employee_table`.\n",
        "2. Write a SQL query to calculate the percentage (%) that each Employee's salary contributes to their respective Department's total salary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e35b41",
      "metadata": {
        "id": "48e35b41"
      },
      "source": [
        "employee_table\n",
        "<img src=\"employees.png\" width=\"300px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Response\n",
        "### 4.1\n",
        "```\n",
        "SELECT\n",
        "  DISTINCT salary\n",
        "FROM\n",
        "  employee_table\n",
        "ORDER BY\n",
        "  salary DESC\n",
        "LIMIT 1 OFFSET 2\n",
        "```\n",
        "### 4.2\n",
        "```\n",
        "SELECT\n",
        "  EmployeeID,\n",
        "  Department,\n",
        "  name,\n",
        "  salary,\n",
        "  (salary / (SELECT SUM(salary) FROM employee_table WHERE Department = e.Department)) * 100 AS salary_percentage\n",
        "FROM\n",
        "  employee_table e\n",
        "ORDER BY\n",
        "  salary_percentage DESC\n",
        "  Department DESC\n",
        "```"
      ],
      "metadata": {
        "id": "9GhD_rM3N0oq"
      },
      "id": "9GhD_rM3N0oq"
    },
    {
      "cell_type": "markdown",
      "id": "38bee427",
      "metadata": {
        "id": "38bee427"
      },
      "source": [
        "# Exercise 5\n",
        "\n",
        "The following sales_daily table is widely used within MMS for multiple analytics.\n",
        "\n",
        "```\n",
        "create table sales_daily\n",
        "(\n",
        "    country          STRING,\n",
        "    sales_date       DATE,\n",
        "    outlet_id        INT64,\n",
        "    product_category INT64,\n",
        "    product_id       INT64,\n",
        "    brand            STRING,\n",
        "    sales_qty        INT64,\n",
        "    sales_value      NUMERIC\n",
        ");\n",
        "```\n",
        "\n",
        "One of our primary use cases within our domain is to analyze the daily sales trends across different countries. However, due to the high volume of products sold each day, these queries consume substantial memory and processing time.\n",
        "- Write a query to retrieve the daily sales evolution per country.\n",
        "- Suppose this query takes too long to run. Explain how we could optimize the previous definition of the table `sales_daily` in order to obtain a better performance in this query.\n",
        "        -> If you think it is needed, do not hesitate to also provide the code for doing so.\n",
        "        -> Taking into account that this table is hosted in BigQuery, adapt the explanation (as much as your can) using BigQuery terminology instead of generic SQL terminology.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Response\n",
        "The query for retrieving the evolution could be:\n",
        "```\n",
        "SELECT\n",
        "  country,\n",
        "  sales_date,\n",
        "  SUM(sales_qty) AS total_sales_qty,\n",
        "  SUM(sales_value) AS total_sales_value\n",
        "FROM\n",
        "  sales_daily\n",
        "GROUP BY\n",
        "  country,\n",
        "  sales_date\n",
        "ORDER BY\n",
        "  country,\n",
        "  sales_date;\n",
        "```\n",
        "This query assumes that sales_value correspond to the value for the whole quantity of items sold, and not to the value of one unit."
      ],
      "metadata": {
        "id": "4w3mCvSKqYOn"
      },
      "id": "4w3mCvSKqYOn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimization explained\n",
        "\n",
        "However, this approach is very expensive in computation, since it processes all the table each time the query is run. In order to avoid this, we should follow the partitioning strategy, alongside with a kpi persistent table:\n",
        "\n",
        "First we modify the table creation query to partition it by date:\n",
        "\n",
        "```\n",
        "CREATE TABLE sales_daily\n",
        "PARTITION BY sales_date\n",
        "(\n",
        "    country          STRING,\n",
        "    sales_date       DATE,\n",
        "    outlet_id        INT64,\n",
        "    product_category INT64,\n",
        "    product_id       INT64,\n",
        "    brand            STRING,\n",
        "    sales_qty        INT64,\n",
        "    sales_value      NUMERIC\n",
        ");\n",
        "```\n",
        "When a table is partitioned in BigQuery and a filter is applied on the query (this can be enforced), we will only be charged for the rows in the partitions being queried.\n",
        "\n",
        "The next step would be creating a table where the daily summary would be inserted:\n",
        "\n",
        "```\n",
        "CREATE TABLE sales_by_country_daily\n",
        "(\n",
        "  country            STRING,\n",
        "  sales_date         DATE,\n",
        "  total_sales_qty    INT64,\n",
        "  total_sales_value  NUMERIC\n",
        ");\n",
        "```\n",
        "\n",
        "Finally, the next step would be to create a job that daily runs the query above, but filtering by a particular date:\n",
        "\n",
        "```\n",
        "MERGE INTO sales_by_country_daily AS target\n",
        "USING (\n",
        "  SELECT\n",
        "    country,\n",
        "    sales_date,\n",
        "    SUM(sales_qty) AS total_sales_qty,\n",
        "    SUM(sales_value) AS total_sales_value\n",
        "  FROM\n",
        "    sales_daily\n",
        "  WHERE\n",
        "    sales_date = DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)\n",
        "  GROUP BY\n",
        "    country,\n",
        "    sales_date\n",
        ") AS source\n",
        "ON target.country = source.country AND target.sales_date = source.sales_date\n",
        "WHEN MATCHED THEN\n",
        "  UPDATE SET\n",
        "    total_sales_qty = source.total_sales_qty,\n",
        "    total_sales_value = source.total_sales_value\n",
        "WHEN NOT MATCHED THEN\n",
        "  INSERT (country, sales_date, total_sales_qty, total_sales_value)\n",
        "  VALUES (source.country, source.sales_date, source.total_sales_qty, source.total_sales_value);\n",
        "```\n",
        "\n",
        "The query above bears in mind that maybe a daily job needs to be rerunned and handles it by updating a row when already in the summary table to avoid duplicates.\n",
        "\n",
        "Note that in the explanation above, I make two assumptions:\n",
        "\n",
        "- Past data is doesn't change.\n",
        "- The table isn't big enough to hit the number of partitions quota.\n",
        "\n",
        "In case past data could change, which is a highly likely scenario since there can occur issues like latency when uploading data to `sales_daily`, there could be two different strategies:\n",
        "\n",
        "  - Simple strategy: Stablishing a hard limit to past data to change based on most likely scenario. If we know that 100% of these cases occurs with data of the past 10 days, we could run each day a query that calculates the summary of the past 10 days instead of only the last day.\n",
        "  - Complex but more cost efficient strategy: Building a system that re-runs the query for a particular date when data is inserted/updated for that date (only if this date is a past one).\n",
        "\n",
        "Regarding the hitting quota scenario, we could stablish a sharding strategy, where we have a table for each year for example. Another strategy would be to partition  by week or month instead of day, but it would incur in addional cost per query so I would discard last option.\n",
        "\n",
        "\n",
        "The last thing I want to mention is clustering. By clustering by country this would increase the speed of the query but won't improve the cost:\n",
        "```\n",
        "CREATE TABLE sales_daily\n",
        "PARTITION BY sales_date\n",
        "CLUSTER BY country\n",
        "(\n",
        "  country          STRING,\n",
        "  sales_date       DATE,\n",
        "  outlet_id        INT64,\n",
        "  product_category INT64,\n",
        "  product_id       INT64,\n",
        "  brand            STRING,\n",
        "  sales_qty        INT64,\n",
        "  sales_value      NUMERIC\n",
        ");\n",
        "```\n",
        "\n",
        "This is due to the fact that BigQuery can take advantage of pre-sorted data within each partition, making the `GROUP BY` more efficient. It won't improve the cost as we always run the query for all countries.\n",
        "\n"
      ],
      "metadata": {
        "id": "5RelP89Bq9DZ"
      },
      "id": "5RelP89Bq9DZ"
    },
    {
      "cell_type": "markdown",
      "id": "37cd2bc1",
      "metadata": {
        "id": "37cd2bc1"
      },
      "source": [
        "# Exercise 6\n",
        "\n",
        "Suppose we have the following tables in a BigQuery.\n",
        "\n",
        "**sales_daily**  \n",
        "Same definition as in previous exercise.\n",
        "\n",
        "\n",
        "**curr_exchange**  \n",
        "This table provides us the exchange rate from the country's local currency to EUR.  \n",
        "Note that the Sales value in `sales_daily` is in local currency. The table has the following structure:\n",
        "```\n",
        "create table curr_exchange\n",
        "(\n",
        "    country       STRING,\n",
        "    ex_loc_to_eur NUMERIC\n",
        ");\n",
        "```\n",
        "\n",
        "\n",
        "**outlets_info**  \n",
        "This table contains relevant information of the outlets and has the following structure:\n",
        "```\n",
        "create table outlets_info\n",
        "(\n",
        "    outlet_id   INT64,\n",
        "    outlet_name STRING,\n",
        "    country     STRING,\n",
        "    region      STRING,\n",
        "    address     STRING,\n",
        "    zipcode     STRING,\n",
        ");\n",
        "```\n",
        "\n",
        "\n",
        "**output_table**  \n",
        "This table is empty and needs to be filled. `sales_value_eur` is the conversion of `sales_value` to EUR.\n",
        "```\n",
        "create table output_table\n",
        "(\n",
        "    country         STRING,\n",
        "    sales_date      DATE,\n",
        "    outlet_id       INT64,\n",
        "    outlet_name     STRING,\n",
        "    region          STRING,\n",
        "    sales_value     NUMERIC,\n",
        "    sales_value_eur NUMERIC\n",
        ");\n",
        "```\n",
        "\n",
        "\n",
        "## Part 1\n",
        "Create a query to fill `output_table`. Join the tables accordingly to create the desired output.\n",
        "\n",
        "\n",
        "## Part 2\n",
        "A new table definition has been added:\n",
        "```\n",
        "create table products_info\n",
        "(\n",
        "    product_id   INT64,\n",
        "    product_name STRING,\n",
        "    is_own_brand BOOLEAN\n",
        ");\n",
        "```\n",
        "This table shows which products have an Own Brand of MediaMarkt (i.e. `OK` or `ISY` are an Own Brand of MediaMarkt).  \n",
        "Modify the query in order to exclude all those products which are Own Brands.\n",
        "\n",
        "\n",
        "## Part 3\n",
        "There is now a new version of the currency exchange table which includes date. As you may know, the currency exchange rates is highly variable. For obtaining the right sales value in eur, we need to use the appropiate date's currency exchange rate. Modify the query to use the new version of the exchange rate table `curr_exchange_v2`.\n",
        "```\n",
        "create table curr_exchange_v2\n",
        "(\n",
        "    country       STRING,\n",
        "    rate_date     DATE,\n",
        "    ex_loc_to_eur NUMERIC\n",
        ");\n",
        "```\n",
        "\n",
        "## Part 4\n",
        "Provide a schema / diagram to illustrate how the tables are combined to produce the `output_table`.\n",
        "\n",
        "\n",
        "## Part 5\n",
        "This query needs to run daily and update `output_table` accordingly. Explain how you would set this up to run every day in GCP. Imagine that `output_table` and the rest of the tables are available in our BigQuery.\n",
        "\n",
        "_Note that: We discard creating a view instead of a table because the response time is too slow due to the size of the sources._  \n",
        "_Note that2: For the scope of this exercise we do not consider the option of creating a materialized view, but rather we want a table that gets updated every day with the results of our previously implemented query._\n",
        "\n",
        "\n",
        "## Part 6\n",
        "As you have seen, the query for filling `output_table` gets constant modifications. The sources we use are in constant re-definitions and improvements. For this reason, we need to adapt and modify this query often. We want to do these modifications under a version controlled (git) environment. Explain how we can set this up.  \n",
        "Ideally:\n",
        "1. Have the SQL code of our query, version controlled in GitHub.\n",
        "2. Once a change is pushed into the master branch, the query needs to be modified in GCP accordingly.\n",
        "3. The next daily run must occurs with the new version of the code.\n",
        "\n",
        "Explain how to implement this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANSWER\n",
        "\n",
        "## Part 1\n",
        "\n",
        "```\n",
        "INSERT INTO output_table (\n",
        "    country,\n",
        "    sales_date,\n",
        "    outlet_id,\n",
        "    outlet_name,\n",
        "    region,\n",
        "    sales_value,\n",
        "    sales_value_eur\n",
        ")\n",
        "SELECT\n",
        "    sales_daily.country,\n",
        "    sales_daily.sales_date,\n",
        "    sales_daily.outlet_id,\n",
        "    outlets_info.outlet_name,\n",
        "    outlets_info.region,\n",
        "    SUM(sales_daily.sales_value) AS sales_value,\n",
        "    SUM(sales_daily.sales_value * curr_exchange.ex_loc_to_eur) AS sales_value_eur\n",
        "FROM\n",
        "    `exercice6.sales_daily` sales_daily\n",
        "JOIN\n",
        "    `exercice6.outlets_info` outlets_info ON sales_daily.outlet_id = outlets_info.outlet_id\n",
        "JOIN\n",
        "    `exercice6.curr_exchange` curr_exchange ON sales_daily.country = curr_exchange.country\n",
        "GROUP BY sales_date, outlet_id, country, outlet_name, region\n",
        "```\n",
        "\n",
        "## Part 2\n",
        "\n",
        "```\n",
        "INSERT INTO output_table (\n",
        "    country,\n",
        "    sales_date,\n",
        "    outlet_id,\n",
        "    outlet_name,\n",
        "    region,\n",
        "    sales_value,\n",
        "    sales_value_eur\n",
        ")\n",
        "SELECT\n",
        "    sales_daily.country,\n",
        "    sales_daily.sales_date,\n",
        "    sales_daily.outlet_id,\n",
        "    outlets_info.outlet_name,\n",
        "    outlets_info.region,\n",
        "    sales_daily.sales_value,\n",
        "    sales_daily.sales_value * curr_exchange.ex_loc_to_eur AS sales_value_eur\n",
        "FROM\n",
        "    sales_daily\n",
        "JOIN\n",
        "    outlets_info ON sales_daily.outlet_id = outlets_info.outlet_id\n",
        "JOIN\n",
        "    curr_exchange ON sales_daily.country = curr_exchange.country\n",
        "JOIN\n",
        "    products_info ON sales_daily.product_id = products_info.product_id\n",
        "WHERE\n",
        "    products_info.is_own_brand = FALSE;\n",
        "```\n",
        "\n",
        "## Part 3\n",
        "```\n",
        "INSERT INTO output_table (\n",
        "    country,\n",
        "    sales_date,\n",
        "    outlet_id,\n",
        "    outlet_name,\n",
        "    region,\n",
        "    sales_value,\n",
        "    sales_value_eur\n",
        ")\n",
        "SELECT\n",
        "    sales_daily.country,\n",
        "    sales_daily.sales_date,\n",
        "    sales_daily.outlet_id,\n",
        "    outlets_info.outlet_name,\n",
        "    outlets_info.region,\n",
        "    SUM(sales_daily.sales_value) AS sales_value,\n",
        "    SUM(sales_daily.sales_value * curr_exchange.ex_loc_to_eur) AS sales_value_eur\n",
        "FROM\n",
        "    `exercice6.sales_daily` sales_daily\n",
        "JOIN\n",
        "    `exercice6.outlets_info` outlets_info ON sales_daily.outlet_id = outlets_info.outlet_id\n",
        "JOIN\n",
        "    `exercice6.curr_exchange_v2` curr_exchange ON sales_daily.country = curr_exchange.country\n",
        "    AND sales_daily.sales_date = curr_exchange.rate_date\n",
        "JOIN\n",
        "    `exercice6.products_info` products_info ON sales_daily.product_id = products_info.product_id\n",
        "WHERE\n",
        "    products_info.is_own_brand is FALSE\n",
        "GROUP BY sales_date, outlet_id, country, outlet_name, region\n",
        "```\n",
        "\n",
        "## Part 4\n",
        "\n",
        "schema\n",
        "<img src=\"https://drive.google.com/file/d/11rZkX3b8yob2ETvwhEpDJkIHZwBIQ1DV/view?usp=sharing\" width=\"300px\"/>\n",
        "\n",
        "\n",
        "## Part 5\n",
        "\n",
        "From my last interview with Sebastian I learned that Dataform is one of the main tools that you use so I will choose Dataform to deploy this job.\n",
        "\n",
        "First step would be defining the model of dataform `output_table.sqlx`:\n",
        "\n",
        "```\n",
        "config {\n",
        "  type: \"incremental\",\n",
        "  name: \"output_table\",\n",
        "  schema: \"exercice6\",\n",
        "  uniqueKey: [\"sales_date\", \"outlet_id\"],\n",
        "  tags: [\"daily_job\"]\n",
        "}\n",
        "SELECT\n",
        "    sales_daily.country,\n",
        "    sales_daily.sales_date,\n",
        "    sales_daily.outlet_id,\n",
        "    outlets_info.outlet_name,\n",
        "    outlets_info.region,\n",
        "    SUM(sales_daily.sales_value) AS sales_value,\n",
        "    SUM(sales_daily.sales_value * curr_exchange.ex_loc_to_eur) AS sales_value_eur\n",
        "FROM\n",
        "    `exercice6.sales_daily` sales_daily\n",
        "JOIN\n",
        "    `exercice6.outlets_info` outlets_info ON sales_daily.outlet_id = outlets_info.outlet_id\n",
        "JOIN\n",
        "    `exercice6.curr_exchange_v2` curr_exchange ON sales_daily.country = curr_exchange.country\n",
        "    AND sales_daily.sales_date = curr_exchange.rate_date\n",
        "JOIN\n",
        "    `exercice6.products_info` products_info ON sales_daily.product_id = products_info.product_id\n",
        "WHERE\n",
        "    products_info.is_own_brand is FALSE\n",
        "GROUP BY sales_date, outlet_id, country, outlet_name, region\n",
        "\n",
        "```\n",
        "\n",
        "Once I have the model, I can deploy it into dataform.\n",
        "\n",
        "## Part 6 CI/CD\n",
        "\n",
        "For this exercise I actually created a real CI/CD pipeline and put it in practice. I invite you to explore the [public repo](https://github.com/Guillethecoder/media_markt_dataform) I created for this.\n",
        "\n",
        "I will now describe the steps I've taken:\n",
        "\n",
        "- Created a GCP project for easy removal of resources once there is no need to keep it alive.\n",
        "- Created a BQ dataset and all raw tables, seeded tables with data generated with AI.\n",
        "- Created a new Dataform repository.\n",
        "- Connected it to my github repo, which required creating a fine-grained password in github and storing it in Secret Manager. Then giving the Dataform SA access to Secret Manager. Finally completing the setup.\n",
        "- Since this is my first time with Dataform for a long time (I've been working with DBT which is pretty similar) I created a Development Workspace to play around, mostly to make sure I understood how to setup the package.json and the dataform.json.\n",
        "- After that, I started working directly in the repo to simulate a real environment. I used GitFlow with main and dev branches, alongside with the feature ones, so I invite you to take a look at the commits and PRs.\n",
        "- Finally I executed the workflow, at first it fails because of the permissions of the SA I used, but after fixing this issue it worked fine and results were as expected ðŸ˜€.\n",
        "\n",
        "I can show you the GCP environment in our next interview. Glad to say this part of the challenge was the most fun."
      ],
      "metadata": {
        "id": "I14DOO_uLJKG"
      },
      "id": "I14DOO_uLJKG"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}